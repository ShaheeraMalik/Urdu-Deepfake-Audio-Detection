{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9c24bf-5025-4f06-a484-d8f3b29f0c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF cache dir: C:\\Users\\LAPTOP MART\\.cache\\huggingface\\datasets\n",
      "Loading dataset from local path…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9b4106364d4984b7a87c87f3a7b094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6794 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5435, Test size: 1359\n",
      "Building features for train_ds…\n",
      "train_ds shapes: X=(5435, 80), y=(5435,)\n",
      "Building features for test_ds…\n",
      "test_ds shapes: X=(1359, 80), y=(1359,)\n",
      "Scaling features…\n",
      "Scaled shapes: X_train=(5435, 80), X_test=(1359, 80)\n",
      "Training SVM…\n",
      "Training Logistic…\n",
      "Training Perceptron…\n",
      "Training Deep Neural Network (MLPClassifier)…\n",
      "\n",
      "Evaluating SVM…\n",
      "\n",
      "--- SVM ---\n",
      "Accuracy : 0.9977924944812362\n",
      "Precision: 0.9970674486803519\n",
      "Recall   : 0.9985315712187959\n",
      "F1-Score : 0.9977989728539985\n",
      "AUC-ROC  : 0.9995365136295314\n",
      "\n",
      "Evaluating Logistic…\n",
      "\n",
      "--- Logistic ---\n",
      "Accuracy : 0.9359823399558499\n",
      "Precision: 0.9230769230769231\n",
      "Recall   : 0.9515418502202643\n",
      "F1-Score : 0.9370932754880694\n",
      "AUC-ROC  : 0.9781078493799246\n",
      "\n",
      "Evaluating Perceptron…\n",
      "\n",
      "--- Perceptron ---\n",
      "Accuracy : 0.8800588668138337\n",
      "Precision: 0.861731843575419\n",
      "Recall   : 0.9060205580029369\n",
      "F1-Score : 0.8833214030064424\n",
      "AUC-ROC  : 0.9551912639316639\n",
      "\n",
      "Evaluating Deep Neural Network (MLPClassifier)…\n",
      "\n",
      "--- Deep Neural Network ---\n",
      "Accuracy : 0.9970566593083149\n",
      "Precision: 0.9985272459499264\n",
      "Recall   : 0.9955947136563876\n",
      "F1-Score : 0.9970588235294118\n",
      "AUC-ROC  : 0.9999306936268458\n",
      "All models and scaler saved as .pkl files.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Part 1: Urdu Deepfake Audio Detection  \n",
    "# Binary classification with SVM, Logistic Regression, Perceptron, and a small DNN.\n",
    "\n",
    "# %%\n",
    "# 0. (Optional) Inspect / override HF cache location\n",
    "import os\n",
    "from datasets import config, load_dataset\n",
    "\n",
    "# Uncomment to override the default cache:\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = r\"D:\\6th Semester\\Data Science\\Assignmentno4\\AudioData\\hf_cache\"\n",
    "\n",
    "print(\"HF cache dir:\", config.HF_DATASETS_CACHE)\n",
    "\n",
    "# %%\n",
    "# 1. Imports and setup\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import pdb      # Python debugger\n",
    "import joblib\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# %%\n",
    "# 2. Load & split dataset from local clone\n",
    "print(\"Loading dataset from local path…\")\n",
    "ds = load_dataset(\n",
    "    \"D:/6th Semester/Data Science/Assignmentno4/deepfake_detection_dataset_urdu\",  # Local folder\n",
    "    \"default\"  # config name\n",
    ")\n",
    "full_train = ds[\"train\"]\n",
    "split      = full_train.train_test_split(test_size=0.2, seed=RANDOM_SEED)\n",
    "train_ds   = split[\"train\"]\n",
    "test_ds    = split[\"test\"]\n",
    "print(f\"Train size: {len(train_ds)}, Test size: {len(test_ds)}\")\n",
    "\n",
    "# %%\n",
    "# 3. Feature extraction: mean+variance of 40 MFCCs\n",
    "def extract_features(audio_array, sr, n_mfcc=40):\n",
    "    mfcc      = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    mfcc_var  = np.var(mfcc, axis=1)\n",
    "    return np.concatenate([mfcc_mean, mfcc_var])\n",
    "\n",
    "# %%\n",
    "# 4. Build X / y arrays (derive y from file path)\n",
    "def build_xy(dataset, name=\"dataset\"):\n",
    "    X, y = [], []\n",
    "    print(f\"Building features for {name}…\")\n",
    "    for ex in dataset:\n",
    "        arr   = ex[\"audio\"][\"array\"]\n",
    "        sr    = ex[\"audio\"][\"sampling_rate\"]\n",
    "        feats = extract_features(arr, sr)\n",
    "        X.append(feats)\n",
    "        # derive label: bonafide→0, else→1\n",
    "        path = ex[\"audio\"][\"path\"].lower()\n",
    "        label = 0 if \"bonafide\" in path else 1\n",
    "        y.append(label)\n",
    "    X_arr = np.array(X)\n",
    "    y_arr = np.array(y)\n",
    "    print(f\"{name} shapes: X={X_arr.shape}, y={y_arr.shape}\")\n",
    "    return X_arr, y_arr\n",
    "\n",
    "X_train, y_train = build_xy(train_ds, \"train_ds\")\n",
    "X_test,  y_test  = build_xy(test_ds,  \"test_ds\")\n",
    "\n",
    "# %%\n",
    "# 5. Scale features\n",
    "print(\"Scaling features…\")\n",
    "scaler         = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "print(f\"Scaled shapes: X_train={X_train_scaled.shape}, X_test={X_test_scaled.shape}\")\n",
    "\n",
    "# %%\n",
    "# 6. Train classical classifiers\n",
    "classifiers = {\n",
    "    \"SVM\":        SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_SEED),\n",
    "    \"Logistic\":   LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "    \"Perceptron\": Perceptron(max_iter=1000, random_state=RANDOM_SEED),\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}…\")\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# %%\n",
    "# 7. Train a “deep” MLP via sklearn\n",
    "print(\"Training Deep Neural Network (MLPClassifier)…\")\n",
    "dnn_clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,32),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=50\n",
    ")\n",
    "dnn_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# %%\n",
    "# 8. Evaluation helper\n",
    "def eval_model(name, y_true, y_pred, y_score):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
    "    print(\"F1-Score :\", f1_score(y_true, y_pred))\n",
    "    print(\"AUC-ROC  :\", roc_auc_score(y_true, y_score))\n",
    "\n",
    "# %%\n",
    "# 9. Evaluate classical models (handle missing predict_proba)\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\nEvaluating {name}…\")\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_score = clf.decision_function(X_test_scaled)\n",
    "    eval_model(name, y_test, y_pred, y_score)\n",
    "\n",
    "# %%\n",
    "# 10. Evaluate the “deep” MLP\n",
    "print(\"\\nEvaluating Deep Neural Network (MLPClassifier)…\")\n",
    "y_pred_dnn   = dnn_clf.predict(X_test_scaled)\n",
    "y_score_dnn  = dnn_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "eval_model(\"Deep Neural Network\", y_test, y_pred_dnn, y_score_dnn)\n",
    "\n",
    "\n",
    "# %% \n",
    "# 11. Save scaler and models to disk for Streamlit app\n",
    "# Replace these names with whatever variables hold your objects\n",
    "# (from the cells above):\n",
    "joblib.dump(scaler,   \"scaler.pkl\")\n",
    "joblib.dump(classifiers[\"SVM\"],        \"svm_clf.pkl\")\n",
    "joblib.dump(classifiers[\"Logistic\"],   \"log_clf.pkl\")\n",
    "joblib.dump(classifiers[\"Perceptron\"], \"per_clf.pkl\")\n",
    "joblib.dump(dnn_clf,    \"dnn_clf.pkl\")\n",
    "\n",
    "print(\"All models and scaler saved as .pkl files.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
